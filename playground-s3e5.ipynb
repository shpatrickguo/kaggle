{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Playground S3E5\n\nThe goal of the notebook is to predict the `quality` of wine using the given data. The dataset for this competition (both train and test) was generated from a deep learning model trained on the [Wine Quality dataset](https://www.kaggle.com/datasets/yasserh/wine-quality-dataset).","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Table of Contents\n<a id=\"toc\"></a>\n- [1. Imports](#1)\n- [2. Data Loading](#2)\n- [3. EDA](#3)\n- [4. Data Cleaning and Processing](#4)   \n- [5. Modelling](#5)\n- [6. Submission](#6)  ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n## Imports","metadata":{}},{"cell_type":"code","source":"%%capture\n# Install extra packages\n!pip install dataprep shutup lazypredict","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:00:01.009471Z","iopub.execute_input":"2023-02-01T13:00:01.009971Z","iopub.status.idle":"2023-02-01T13:00:55.872245Z","shell.execute_reply.started":"2023-02-01T13:00:01.009933Z","shell.execute_reply":"2023-02-01T13:00:55.870092Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pandas_profiling\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom dataprep.eda import create_report\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import RobustScaler\nfrom lazypredict.Supervised import LazyClassifier\nimport optuna\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import make_scorer, cohen_kappa_score\n\n\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\nimport shutup\nshutup.please()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:35:54.337210Z","iopub.execute_input":"2023-02-01T13:35:54.337683Z","iopub.status.idle":"2023-02-01T13:35:54.347126Z","shell.execute_reply.started":"2023-02-01T13:35:54.337645Z","shell.execute_reply":"2023-02-01T13:35:54.345961Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# Load Data\n\nThis datasets is related to red variants of the Portuguese \"Vinho Verde\" wine. The dataset describes the amount of various chemicals present in wine and their effect on it's quality\n\nInput variables (based on physicochemical tests):\\\n1 - fixed acidity\\\n2 - volatile acidity\\\n3 - citric acid\\\n4 - residual sugar\\\n5 - chlorides\\\n6 - free sulfur dioxide\\\n7 - total sulfur dioxide\\\n8 - density\\\n9 - pH\\\n10 - sulphates\\\n11 - alcohol\\\nOutput variable (based on sensory data):\\\n12 - quality (score between 0 and 10)\n\nAcknowledgements:\n\nThe original  dataset is also available from Kaggle & UCI machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality.\n\n**Acknowledgements**: P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s3e5/train.csv', index_col=0)\ntest = pd.read_csv('/kaggle/input/playground-series-s3e5/test.csv', index_col=0)\nsubmission = pd.read_csv('/kaggle/input/playground-series-s3e5/sample_submission.csv', index_col=0)\n# Load original dataset\noriginal = pd.read_csv('/kaggle/input/wine-quality-dataset/WineQT.csv', index_col=\"Id\")\n\nprint('Train shape:            ', train.shape)\nprint('Test shape:             ', test.shape)\nprint('Original Dataset shape: ', original.shape)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:01:01.200009Z","iopub.execute_input":"2023-02-01T13:01:01.201552Z","iopub.status.idle":"2023-02-01T13:01:01.254236Z","shell.execute_reply.started":"2023-02-01T13:01:01.201515Z","shell.execute_reply":"2023-02-01T13:01:01.253013Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Train shape:             (2056, 12)\nTest shape:              (1372, 11)\nOriginal Dataset shape:  (1143, 12)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" > Back to Table of Contents </a>","metadata":{"execution":{"iopub.status.busy":"2023-01-31T06:15:42.344699Z","iopub.execute_input":"2023-01-31T06:15:42.345217Z","iopub.status.idle":"2023-01-31T06:15:42.373241Z","shell.execute_reply.started":"2023-01-31T06:15:42.345115Z","shell.execute_reply":"2023-01-31T06:15:42.371604Z"}}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n## EDA\n\nAfter concating `train.csv` and `WineQT.csv`:\n- There are 12 columns and 3199 rows.\n- 11 continous features and 1 target column\n- `quality` is the target variable\n- No missing values\n- 125 duplicates that will be dropped\n- Target value `quality` distribution is between 3-8, with large data imbalance (many 5 and 6s, very few 3, 4, and 8)\n- Density and citric acid are positively correlated to fixed acidity\n- Total and Free sulfer dioxide is positively correlated\n- pH and fixed acidity is negatively correlated","metadata":{}},{"cell_type":"code","source":"# Combine train and original dataste\ntrain = pd.concat([train, original], axis=0, ignore_index=True)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:01:01.257486Z","iopub.execute_input":"2023-02-01T13:01:01.258487Z","iopub.status.idle":"2023-02-01T13:01:01.277717Z","shell.execute_reply.started":"2023-02-01T13:01:01.258451Z","shell.execute_reply":"2023-02-01T13:01:01.276749Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0           8.00              0.50         0.39            2.20       0.07   \n1           9.30              0.30         0.73            2.30       0.09   \n2           7.10              0.51         0.03            2.10       0.06   \n3           8.10              0.87         0.22            2.60       0.08   \n4           8.50              0.36         0.30            2.30       0.08   \n\n   free sulfur dioxide  total sulfur dioxide  density   pH  sulphates  \\\n0                30.00                 39.00     1.00 3.33       0.77   \n1                30.00                 67.00     1.00 3.32       0.67   \n2                 3.00                 12.00     1.00 3.52       0.73   \n3                11.00                 65.00     1.00 3.20       0.53   \n4                10.00                 45.00     0.99 3.20       1.36   \n\n   alcohol  quality  \n0    12.10        6  \n1    12.80        6  \n2    11.30        7  \n3     9.80        5  \n4     9.50        6  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8.00</td>\n      <td>0.50</td>\n      <td>0.39</td>\n      <td>2.20</td>\n      <td>0.07</td>\n      <td>30.00</td>\n      <td>39.00</td>\n      <td>1.00</td>\n      <td>3.33</td>\n      <td>0.77</td>\n      <td>12.10</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9.30</td>\n      <td>0.30</td>\n      <td>0.73</td>\n      <td>2.30</td>\n      <td>0.09</td>\n      <td>30.00</td>\n      <td>67.00</td>\n      <td>1.00</td>\n      <td>3.32</td>\n      <td>0.67</td>\n      <td>12.80</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.10</td>\n      <td>0.51</td>\n      <td>0.03</td>\n      <td>2.10</td>\n      <td>0.06</td>\n      <td>3.00</td>\n      <td>12.00</td>\n      <td>1.00</td>\n      <td>3.52</td>\n      <td>0.73</td>\n      <td>11.30</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8.10</td>\n      <td>0.87</td>\n      <td>0.22</td>\n      <td>2.60</td>\n      <td>0.08</td>\n      <td>11.00</td>\n      <td>65.00</td>\n      <td>1.00</td>\n      <td>3.20</td>\n      <td>0.53</td>\n      <td>9.80</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8.50</td>\n      <td>0.36</td>\n      <td>0.30</td>\n      <td>2.30</td>\n      <td>0.08</td>\n      <td>10.00</td>\n      <td>45.00</td>\n      <td>0.99</td>\n      <td>3.20</td>\n      <td>1.36</td>\n      <td>9.50</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:01:01.281191Z","iopub.execute_input":"2023-02-01T13:01:01.281519Z","iopub.status.idle":"2023-02-01T13:01:01.327473Z","shell.execute_reply.started":"2023-02-01T13:01:01.281490Z","shell.execute_reply":"2023-02-01T13:01:01.326337Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"       fixed acidity  volatile acidity  citric acid  residual sugar  \\\ncount        3199.00           3199.00      3199.00         3199.00   \nmean            8.35              0.53         0.27            2.45   \nstd             1.72              0.18         0.19            1.07   \nmin             4.60              0.12         0.00            0.90   \n25%             7.10              0.39         0.09            1.90   \n50%             7.90              0.52         0.25            2.20   \n75%             9.10              0.64         0.42            2.60   \nmax            15.90              1.58         1.00           15.50   \n\n       chlorides  free sulfur dioxide  total sulfur dioxide  density      pH  \\\ncount    3199.00              3199.00               3199.00  3199.00 3199.00   \nmean        0.08                16.48                 48.05     1.00    3.31   \nstd         0.03                10.12                 32.93     0.00    0.15   \nmin         0.01                 1.00                  6.00     0.99    2.74   \n25%         0.07                 7.00                 22.00     1.00    3.20   \n50%         0.08                15.00                 42.00     1.00    3.31   \n75%         0.09                23.00                 64.00     1.00    3.39   \nmax         0.61                68.00                289.00     1.00    4.01   \n\n       sulphates  alcohol  quality  \ncount    3199.00  3199.00  3199.00  \nmean        0.65    10.42     5.70  \nstd         0.15     1.05     0.84  \nmin         0.33     8.40     3.00  \n25%         0.55     9.50     5.00  \n50%         0.62    10.10     6.00  \n75%         0.72    11.10     6.00  \nmax         2.00    14.90     8.00  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3199.00</td>\n      <td>3199.00</td>\n      <td>3199.00</td>\n      <td>3199.00</td>\n      <td>3199.00</td>\n      <td>3199.00</td>\n      <td>3199.00</td>\n      <td>3199.00</td>\n      <td>3199.00</td>\n      <td>3199.00</td>\n      <td>3199.00</td>\n      <td>3199.00</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>8.35</td>\n      <td>0.53</td>\n      <td>0.27</td>\n      <td>2.45</td>\n      <td>0.08</td>\n      <td>16.48</td>\n      <td>48.05</td>\n      <td>1.00</td>\n      <td>3.31</td>\n      <td>0.65</td>\n      <td>10.42</td>\n      <td>5.70</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.72</td>\n      <td>0.18</td>\n      <td>0.19</td>\n      <td>1.07</td>\n      <td>0.03</td>\n      <td>10.12</td>\n      <td>32.93</td>\n      <td>0.00</td>\n      <td>0.15</td>\n      <td>0.15</td>\n      <td>1.05</td>\n      <td>0.84</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>4.60</td>\n      <td>0.12</td>\n      <td>0.00</td>\n      <td>0.90</td>\n      <td>0.01</td>\n      <td>1.00</td>\n      <td>6.00</td>\n      <td>0.99</td>\n      <td>2.74</td>\n      <td>0.33</td>\n      <td>8.40</td>\n      <td>3.00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>7.10</td>\n      <td>0.39</td>\n      <td>0.09</td>\n      <td>1.90</td>\n      <td>0.07</td>\n      <td>7.00</td>\n      <td>22.00</td>\n      <td>1.00</td>\n      <td>3.20</td>\n      <td>0.55</td>\n      <td>9.50</td>\n      <td>5.00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>7.90</td>\n      <td>0.52</td>\n      <td>0.25</td>\n      <td>2.20</td>\n      <td>0.08</td>\n      <td>15.00</td>\n      <td>42.00</td>\n      <td>1.00</td>\n      <td>3.31</td>\n      <td>0.62</td>\n      <td>10.10</td>\n      <td>6.00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>9.10</td>\n      <td>0.64</td>\n      <td>0.42</td>\n      <td>2.60</td>\n      <td>0.09</td>\n      <td>23.00</td>\n      <td>64.00</td>\n      <td>1.00</td>\n      <td>3.39</td>\n      <td>0.72</td>\n      <td>11.10</td>\n      <td>6.00</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>15.90</td>\n      <td>1.58</td>\n      <td>1.00</td>\n      <td>15.50</td>\n      <td>0.61</td>\n      <td>68.00</td>\n      <td>289.00</td>\n      <td>1.00</td>\n      <td>4.01</td>\n      <td>2.00</td>\n      <td>14.90</td>\n      <td>8.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#report = create_report(train)\n#report.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:01:01.328706Z","iopub.execute_input":"2023-02-01T13:01:01.329044Z","iopub.status.idle":"2023-02-01T13:01:01.333403Z","shell.execute_reply.started":"2023-02-01T13:01:01.329016Z","shell.execute_reply":"2023-02-01T13:01:01.332589Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" > Back to Table of Contents </a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n## Data Cleaning and Processing\n\n- Label Encode target variable\n- Drop duplicates\n- Train Test Split\n- Over-sampling with SMOTE\n- Normalization ","metadata":{}},{"cell_type":"code","source":"train = train.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:01:01.334645Z","iopub.execute_input":"2023-02-01T13:01:01.335698Z","iopub.status.idle":"2023-02-01T13:01:01.351502Z","shell.execute_reply.started":"2023-02-01T13:01:01.335667Z","shell.execute_reply":"2023-02-01T13:01:01.350268Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X = train.drop('quality', axis=1)\n\n# Create LabelEncoder object\nle = LabelEncoder()\ny = le.fit_transform(train['quality'])\n\nX.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:01:01.353208Z","iopub.execute_input":"2023-02-01T13:01:01.353659Z","iopub.status.idle":"2023-02-01T13:01:01.363044Z","shell.execute_reply.started":"2023-02-01T13:01:01.353628Z","shell.execute_reply":"2023-02-01T13:01:01.361813Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"((3074, 11), (3074,))"},"metadata":{}}]},{"cell_type":"markdown","source":"SMOTE (Synthetic Minority Over-sampling Technique) is an oversampling technique used to balance class distribution in binary or multi-class classification problems. The goal of oversampling is to increase the number of instances of the minority class to address class imbalance. ","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:01:01.364755Z","iopub.execute_input":"2023-02-01T13:01:01.365227Z","iopub.status.idle":"2023-02-01T13:01:01.385379Z","shell.execute_reply.started":"2023-02-01T13:01:01.365187Z","shell.execute_reply":"2023-02-01T13:01:01.383759Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"((2459, 11), (615, 11), (2459,), (615,))"},"metadata":{}}]},{"cell_type":"code","source":"def balance_data_using_smote(X_train, y_train):\n    smote = SMOTE(sampling_strategy='minority', random_state=42)\n    X_smote, y_smote = smote.fit_resample(X_train, y_train)\n    return X_smote, y_smote\n\nX_train, y_train = balance_data_using_smote(X_train, y_train)\nX_train.shape, y_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:01:01.389576Z","iopub.execute_input":"2023-02-01T13:01:01.390172Z","iopub.status.idle":"2023-02-01T13:01:01.416626Z","shell.execute_reply.started":"2023-02-01T13:01:01.390123Z","shell.execute_reply":"2023-02-01T13:01:01.415301Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"((3445, 11), (3445,))"},"metadata":{}}]},{"cell_type":"code","source":"def apply_robust_scaler(train, test):\n    robust_scaler = RobustScaler()\n    train_scaled = robust_scaler.fit_transform(train)\n    test_scaled = robust_scaler.transform(test)\n    return train_scaled, test_scaled\n\nX_train, X_test = apply_robust_scaler(X_train, X_test)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:01:01.418376Z","iopub.execute_input":"2023-02-01T13:01:01.419228Z","iopub.status.idle":"2023-02-01T13:01:01.438045Z","shell.execute_reply.started":"2023-02-01T13:01:01.419183Z","shell.execute_reply":"2023-02-01T13:01:01.436249Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" > Back to Table of Contents </a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n## Modelling\n\n\n### Evaluating models\n\nQuadratic Weighted Kappa (QWK) is a scoring metric for evaluating the agreement between two ratings. It measures the agreement between two raters, who assign categorical ratings to a number of items. Unlike simple accuracy or mean squared error, QWK takes into account the agreement beyond chance, considering both the amount and order of agreement. It ranges from -1 (representing complete disagreement) to 1 (representing complete agreement).\n\nThe metric is useful in these cases as it takes into account the possibility of different raters assigning similar scores to different conditions, while still penalizing substantial disagreements.\n\nThe quadratic weighted kappa is calculated as follows. First, an $N x N$ histogram matrix $O$ is constructed, such that $O_{i,j}$ corresponds to the number of `Ids` $i$ (actual) that received a predicted value $j$. An $N$-by-$N$ matrix of weights, $w$, is calculated based on the difference between actual and predicted values:\n\n$$ w_{i,j} = \\frac{(i-j)^2}{(N-1)^2} $$\n\nAn $N$-by-$N$ histogram matrix of expected outcomes, $E$, is calculated assuming that there is no correlation between values.  This is calculated as the outer product between the actual histogram vector of outcomes and the predicted histogram vector, normalized such that $E$ and $O$ have the same sum.\n\nFrom these three matrices, the quadratic weighted kappa is calculated as: \n\n$$ k = 1 - \\frac{\\sum_{i,j} w_{i,j} O_{i,j}}{\\sum_{i,j} w_{i,j} E_{i,j}} $$","metadata":{}},{"cell_type":"code","source":"# Define the scoring metric as quadratic_weighted_kappa\nkappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:01:01.439812Z","iopub.execute_input":"2023-02-01T13:01:01.440449Z","iopub.status.idle":"2023-02-01T13:01:01.444490Z","shell.execute_reply.started":"2023-02-01T13:01:01.440413Z","shell.execute_reply":"2023-02-01T13:01:01.443619Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#maker_scorer isn't supported for LazyPredict, so we'll make do with using default cohen_kappa_score as indicator\nclf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=cohen_kappa_score, random_state=42) \nmodels, predictions = clf.fit(X_train, X_test, y_train, y_test)\nmodels.sort_values(\"cohen_kappa_score\", ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:01:01.445698Z","iopub.execute_input":"2023-02-01T13:01:01.446511Z","iopub.status.idle":"2023-02-01T13:01:16.140165Z","shell.execute_reply.started":"2023-02-01T13:01:01.446471Z","shell.execute_reply":"2023-02-01T13:01:16.138967Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 29/29 [00:14<00:00,  1.98it/s]\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                               Accuracy  Balanced Accuracy ROC AUC  F1 Score  \\\nModel                                                                          \nLGBMClassifier                     0.60               0.29    None      0.58   \nXGBClassifier                      0.58               0.28    None      0.56   \nRandomForestClassifier             0.58               0.27    None      0.56   \nExtraTreesClassifier               0.58               0.27    None      0.56   \nSVC                                0.56               0.29    None      0.55   \nBaggingClassifier                  0.54               0.26    None      0.53   \nQuadraticDiscriminantAnalysis      0.52               0.27    None      0.52   \nLogisticRegression                 0.46               0.36    None      0.48   \nLinearDiscriminantAnalysis         0.45               0.36    None      0.48   \nDecisionTreeClassifier             0.48               0.26    None      0.49   \nKNeighborsClassifier               0.49               0.28    None      0.49   \nGaussianNB                         0.41               0.30    None      0.46   \nLabelPropagation                   0.46               0.27    None      0.47   \nBernoulliNB                        0.42               0.29    None      0.45   \nCalibratedClassifierCV             0.44               0.32    None      0.45   \nLabelSpreading                     0.46               0.25    None      0.46   \nRidgeClassifier                    0.42               0.31    None      0.43   \nRidgeClassifierCV                  0.42               0.31    None      0.43   \nLinearSVC                          0.41               0.31    None      0.42   \nPassiveAggressiveClassifier        0.35               0.31    None      0.38   \nExtraTreeClassifier                0.42               0.28    None      0.43   \nNearestCentroid                    0.28               0.42    None      0.33   \nSGDClassifier                      0.37               0.30    None      0.40   \nPerceptron                         0.36               0.28    None      0.39   \nAdaBoostClassifier                 0.21               0.33    None      0.24   \nDummyClassifier                    0.01               0.17    None      0.00   \n\n                               cohen_kappa_score  Time Taken  \nModel                                                         \nLGBMClassifier                              0.35        1.00  \nXGBClassifier                               0.33        2.46  \nRandomForestClassifier                      0.33        1.08  \nExtraTreesClassifier                        0.32        0.54  \nSVC                                         0.31        0.60  \nBaggingClassifier                           0.27        0.30  \nQuadraticDiscriminantAnalysis               0.26        0.03  \nLogisticRegression                          0.23        0.14  \nLinearDiscriminantAnalysis                  0.22        0.09  \nDecisionTreeClassifier                      0.22        0.06  \nKNeighborsClassifier                        0.21        0.06  \nGaussianNB                                  0.20        0.01  \nLabelPropagation                            0.19        0.59  \nBernoulliNB                                 0.19        0.01  \nCalibratedClassifierCV                      0.18        4.78  \nLabelSpreading                              0.18        0.86  \nRidgeClassifier                             0.16        0.02  \nRidgeClassifierCV                           0.16        0.04  \nLinearSVC                                   0.15        1.32  \nPassiveAggressiveClassifier                 0.13        0.03  \nExtraTreeClassifier                         0.13        0.02  \nNearestCentroid                             0.13        0.01  \nSGDClassifier                               0.12        0.15  \nPerceptron                                  0.11        0.03  \nAdaBoostClassifier                          0.07        0.32  \nDummyClassifier                             0.00        0.01  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n      <th>Balanced Accuracy</th>\n      <th>ROC AUC</th>\n      <th>F1 Score</th>\n      <th>cohen_kappa_score</th>\n      <th>Time Taken</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LGBMClassifier</th>\n      <td>0.60</td>\n      <td>0.29</td>\n      <td>None</td>\n      <td>0.58</td>\n      <td>0.35</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>XGBClassifier</th>\n      <td>0.58</td>\n      <td>0.28</td>\n      <td>None</td>\n      <td>0.56</td>\n      <td>0.33</td>\n      <td>2.46</td>\n    </tr>\n    <tr>\n      <th>RandomForestClassifier</th>\n      <td>0.58</td>\n      <td>0.27</td>\n      <td>None</td>\n      <td>0.56</td>\n      <td>0.33</td>\n      <td>1.08</td>\n    </tr>\n    <tr>\n      <th>ExtraTreesClassifier</th>\n      <td>0.58</td>\n      <td>0.27</td>\n      <td>None</td>\n      <td>0.56</td>\n      <td>0.32</td>\n      <td>0.54</td>\n    </tr>\n    <tr>\n      <th>SVC</th>\n      <td>0.56</td>\n      <td>0.29</td>\n      <td>None</td>\n      <td>0.55</td>\n      <td>0.31</td>\n      <td>0.60</td>\n    </tr>\n    <tr>\n      <th>BaggingClassifier</th>\n      <td>0.54</td>\n      <td>0.26</td>\n      <td>None</td>\n      <td>0.53</td>\n      <td>0.27</td>\n      <td>0.30</td>\n    </tr>\n    <tr>\n      <th>QuadraticDiscriminantAnalysis</th>\n      <td>0.52</td>\n      <td>0.27</td>\n      <td>None</td>\n      <td>0.52</td>\n      <td>0.26</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>LogisticRegression</th>\n      <td>0.46</td>\n      <td>0.36</td>\n      <td>None</td>\n      <td>0.48</td>\n      <td>0.23</td>\n      <td>0.14</td>\n    </tr>\n    <tr>\n      <th>LinearDiscriminantAnalysis</th>\n      <td>0.45</td>\n      <td>0.36</td>\n      <td>None</td>\n      <td>0.48</td>\n      <td>0.22</td>\n      <td>0.09</td>\n    </tr>\n    <tr>\n      <th>DecisionTreeClassifier</th>\n      <td>0.48</td>\n      <td>0.26</td>\n      <td>None</td>\n      <td>0.49</td>\n      <td>0.22</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>KNeighborsClassifier</th>\n      <td>0.49</td>\n      <td>0.28</td>\n      <td>None</td>\n      <td>0.49</td>\n      <td>0.21</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>GaussianNB</th>\n      <td>0.41</td>\n      <td>0.30</td>\n      <td>None</td>\n      <td>0.46</td>\n      <td>0.20</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LabelPropagation</th>\n      <td>0.46</td>\n      <td>0.27</td>\n      <td>None</td>\n      <td>0.47</td>\n      <td>0.19</td>\n      <td>0.59</td>\n    </tr>\n    <tr>\n      <th>BernoulliNB</th>\n      <td>0.42</td>\n      <td>0.29</td>\n      <td>None</td>\n      <td>0.45</td>\n      <td>0.19</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>CalibratedClassifierCV</th>\n      <td>0.44</td>\n      <td>0.32</td>\n      <td>None</td>\n      <td>0.45</td>\n      <td>0.18</td>\n      <td>4.78</td>\n    </tr>\n    <tr>\n      <th>LabelSpreading</th>\n      <td>0.46</td>\n      <td>0.25</td>\n      <td>None</td>\n      <td>0.46</td>\n      <td>0.18</td>\n      <td>0.86</td>\n    </tr>\n    <tr>\n      <th>RidgeClassifier</th>\n      <td>0.42</td>\n      <td>0.31</td>\n      <td>None</td>\n      <td>0.43</td>\n      <td>0.16</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>RidgeClassifierCV</th>\n      <td>0.42</td>\n      <td>0.31</td>\n      <td>None</td>\n      <td>0.43</td>\n      <td>0.16</td>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>LinearSVC</th>\n      <td>0.41</td>\n      <td>0.31</td>\n      <td>None</td>\n      <td>0.42</td>\n      <td>0.15</td>\n      <td>1.32</td>\n    </tr>\n    <tr>\n      <th>PassiveAggressiveClassifier</th>\n      <td>0.35</td>\n      <td>0.31</td>\n      <td>None</td>\n      <td>0.38</td>\n      <td>0.13</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>ExtraTreeClassifier</th>\n      <td>0.42</td>\n      <td>0.28</td>\n      <td>None</td>\n      <td>0.43</td>\n      <td>0.13</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>NearestCentroid</th>\n      <td>0.28</td>\n      <td>0.42</td>\n      <td>None</td>\n      <td>0.33</td>\n      <td>0.13</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>SGDClassifier</th>\n      <td>0.37</td>\n      <td>0.30</td>\n      <td>None</td>\n      <td>0.40</td>\n      <td>0.12</td>\n      <td>0.15</td>\n    </tr>\n    <tr>\n      <th>Perceptron</th>\n      <td>0.36</td>\n      <td>0.28</td>\n      <td>None</td>\n      <td>0.39</td>\n      <td>0.11</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>AdaBoostClassifier</th>\n      <td>0.21</td>\n      <td>0.33</td>\n      <td>None</td>\n      <td>0.24</td>\n      <td>0.07</td>\n      <td>0.32</td>\n    </tr>\n    <tr>\n      <th>DummyClassifier</th>\n      <td>0.01</td>\n      <td>0.17</td>\n      <td>None</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Model Tuning\n\nThe performance of a machine learning model is highly dependent on the choice of hyperparameters. Hyperparameters determine the architecture and behavior of a model and can greatly affect the model's accuracy, precision, recall, and overall performance.","metadata":{}},{"cell_type":"code","source":"def tune_extra_trees_classifier(X_train, y_train, n_splits=5, trials=100):\n    def objective(trial):\n        params = {\n        \"n_estimators\": trial.suggest_int('n_estimators', 50, 500),\n        \"max_depth\": trial.suggest_int('max_depth', 2, 30),\n        \"min_samples_split\": trial.suggest_int('min_samples_split', 2, 20),\n        \"min_samples_leaf\": trial.suggest_int('min_samples_leaf', 1, 20),\n        \"max_features\": trial.suggest_uniform('max_features', 0.1, 1.0)\n        }\n        model = ExtraTreesClassifier(**params, random_state=42)\n        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n        kappa_scores = -1 * cross_val_score(model, X, y, cv=cv, scoring=kappa_scorer)\n        return np.mean(kappa_scores)\n    \n    study = optuna.create_study()\n    study.optimize(objective, n_trials=trials)\n    best_params = study.best_params\n    best_score = study.best_value\n    return best_params, best_score\n\ndef tune_lgbm(X_train, y_train, n_splits=5, trials=100):\n    def objective(trial):\n        params = {\n            \"objective\" : 'multiclass',\n            \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"dart\", \"goss\"]),\n            \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 100),\n            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1.0),\n            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n            \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 1.0),\n            \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 1.0),\n        }\n        model = lgb.LGBMClassifier(**params)\n        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n        kappa_scores = -1 * cross_val_score(model, X, y, cv=cv, scoring=kappa_scorer)\n        return np.mean(kappa_scores)\n    \n    study = optuna.create_study()\n    study.optimize(objective, n_trials=trials)\n    best_params = study.best_params\n    best_score = study.best_value\n    return best_params, best_score\n\ndef tune_xgb_classifier(X, y, n_splits=5, trials=100):\n    def objective(trial):\n        xgb_params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'max_depth': trial.suggest_int('max_depth', 3, 20),\n            'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.3),\n            'subsample': trial.suggest_uniform('subsample', 0.3, 1.0),\n            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.3, 1.0),\n            'reg_alpha': trial.suggest_uniform('reg_alpha', 0.0, 1.0),\n            'reg_lambda': trial.suggest_uniform('reg_lambda', 0.0, 1.0),\n            'objective': 'multi:softmax',\n            'num_class': len(np.unique(y))\n        }\n        model = xgb.XGBClassifier(**xgb_params)\n        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n        kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')\n        kappa_scores = -1 * cross_val_score(model, X, y, cv=cv, scoring=kappa_scorer)\n        return np.mean(kappa_scores)\n\n    study = optuna.create_study()\n    study.optimize(objective, n_trials=trials)\n    best_params = study.best_params\n    best_value = study.best_value\n    return best_params, best_value\n\ndef tune_rf_classifier(X, y, n_splits=5, trials=100):\n    def objective(trial):\n        rf_params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'max_depth': trial.suggest_int('max_depth', 1, 30),\n            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n            'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n        }\n        model = RandomForestClassifier(**rf_params)\n        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n        kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')\n        kappa_scores = -1 * cross_val_score(model, X, y, cv=cv, scoring=kappa_scorer)\n        return np.mean(kappa_scores)\n\n    study = optuna.create_study()\n    study.optimize(objective, n_trials=trials)\n    best_params = study.best_params\n    best_value = study.best_value\n    return best_params, best_value","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:42:26.542282Z","iopub.execute_input":"2023-02-01T13:42:26.542747Z","iopub.status.idle":"2023-02-01T13:42:26.569356Z","shell.execute_reply.started":"2023-02-01T13:42:26.542711Z","shell.execute_reply":"2023-02-01T13:42:26.568028Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Tune hyper-parameters using optuna\n#tune_extra_trees_classifier(X_train, y_train, n_splits=5, trials=100)\n#tune_lgbm(X_train, y_train, n_splits=5, trials=1)\n#tune_xgb_classifier(X_train, y_train, n_splits=5, trials=100)\n#tune_rf_classifier(X_train, y_train, n_splits=5, trials=100)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:11:32.680395Z","iopub.execute_input":"2023-02-01T23:11:32.680829Z","iopub.status.idle":"2023-02-01T23:11:32.685234Z","shell.execute_reply.started":"2023-02-01T23:11:32.680797Z","shell.execute_reply":"2023-02-01T23:11:32.684247Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# If params not defined, used hard-coded values from previous hyper-parameter tuning\nif 'trees_classifier_params' not in locals():\n    trees_classifier_params = {\n        'n_estimators': 483,\n        'max_depth': 20,\n        'min_samples_split': 9,\n        'min_samples_leaf': 1,\n        'max_features': 0.9760192595957563\n    }\n\nif 'lgbm_params' not in locals():\n    lgbm_params = {\n        'boosting_type': 'gbdt',\n        'num_leaves': 90,\n        'learning_rate': 1.2846185076835063e-05,\n        'n_estimators': 568,\n        'min_child_weight': 4,\n        'subsample': 0.2078432609415698,\n        'colsample_bytree': 0.9785173128446786\n    }\n\nif 'xgb_params' not in locals():\n    xgb_params = {\n        'n_estimators': 515,\n        'max_depth': 12,\n        'learning_rate': 0.04876788920845744,\n        'subsample': 0.5820311109870313,\n        'colsample_bytree': 0.34985037004316355,\n        'reg_alpha': 0.34472042964865496,\n        'reg_lambda': 0.11032246179781986\n    }\n\nif 'rf_params' not in locals():  \n    rf_params = {\n        'n_estimators': 968,\n        'max_depth': 25,\n        'min_samples_split': 14,\n        'min_samples_leaf': 1,\n        'max_features': 0.7048284037565753\n    }","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:11:24.880183Z","iopub.execute_input":"2023-02-01T23:11:24.880640Z","iopub.status.idle":"2023-02-01T23:11:24.889606Z","shell.execute_reply.started":"2023-02-01T23:11:24.880600Z","shell.execute_reply":"2023-02-01T23:11:24.888781Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"xgb_params = {          \n    'subsample'       : 0.1,\n    'reg_lambda'      : 50,\n    'min_child_weight': 1,\n    'max_depth'       : 6,\n    'learning_rate'   : 0.05,\n    'colsample_bytree': 0.4,\n    'objective'       : 'multi:softmax',\n    'eval_metric'     : 'mlogloss',\n}","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:31:03.248488Z","iopub.execute_input":"2023-02-01T12:31:03.249304Z","iopub.status.idle":"2023-02-01T12:31:03.257278Z","shell.execute_reply.started":"2023-02-01T12:31:03.249269Z","shell.execute_reply":"2023-02-01T12:31:03.256455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" > Back to Table of Contents </a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n## Submssion","metadata":{}},{"cell_type":"code","source":"X, y = balance_data_using_smote(X, y)\nX, test = apply_robust_scaler(X, test)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:31:03.260122Z","iopub.execute_input":"2023-02-01T12:31:03.261232Z","iopub.status.idle":"2023-02-01T12:31:03.281245Z","shell.execute_reply.started":"2023-02-01T12:31:03.261198Z","shell.execute_reply":"2023-02-01T12:31:03.280397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = xgb.XGBClassifier(**xgb_params)\nmodel.fit(X, y)\ny_pred = model.predict(test)\n# Reverse the encoding\npredictions = le.inverse_transform(y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:31:03.282630Z","iopub.execute_input":"2023-02-01T12:31:03.283039Z","iopub.status.idle":"2023-02-01T12:31:04.072505Z","shell.execute_reply.started":"2023-02-01T12:31:03.283003Z","shell.execute_reply":"2023-02-01T12:31:04.071703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert len(submission) == len(predictions), \"Length of submission DataFrame and predictions array are unequal\"\n\nsubmission.quality = predictions\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:31:04.076484Z","iopub.execute_input":"2023-02-01T12:31:04.078601Z","iopub.status.idle":"2023-02-01T12:31:04.089036Z","shell.execute_reply.started":"2023-02-01T12:31:04.078564Z","shell.execute_reply":"2023-02-01T12:31:04.088152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:31:04.090724Z","iopub.execute_input":"2023-02-01T12:31:04.091347Z","iopub.status.idle":"2023-02-01T12:31:04.100116Z","shell.execute_reply.started":"2023-02-01T12:31:04.091306Z","shell.execute_reply":"2023-02-01T12:31:04.099093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}